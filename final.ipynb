{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "config.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "def str2bool(v):\n",
    "    if v.lower() in ('yes', 'true', 't', 'y', '1', True):\n",
    "        return True\n",
    "    elif v.lower() in ('no', 'false', 'f', 'n', '0', False):\n",
    "        return False\n",
    "    else:\n",
    "        raise argparse.ArgumentTypeError('Boolean value expected.')\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(description='')\n",
    "\n",
    "#Image setting\n",
    "image_size = 128\n",
    "mask_size = int(image_size/2)\n",
    "\n",
    "parser.add_argument('--GPU', type=int, default=0)\n",
    "parser.add_argument('--CROP_SIZE', type=int, default=image_size+128) # 178\n",
    "parser.add_argument('--IMG_SIZE', type=int, default=image_size)\n",
    "\n",
    "parser.add_argument('--G_LR', type=float, default=1e-4)\n",
    "parser.add_argument('--D_LR', type=float, default=1e-4)\n",
    "parser.add_argument('--GLOBAL_WGAN_LOSS_ALPHA', type=float, default=1.)\n",
    "parser.add_argument('--WGAN_GP_LAMBDA', type=float, default=10)\n",
    "parser.add_argument('--GAN_LOSS_ALPHA', type=float, default=0.001)\n",
    "parser.add_argument('--COARSE_L1_ALPHA', type=float, default=2)\n",
    "parser.add_argument('--L1_LOSS_ALPHA', type=float, default=2)\n",
    "parser.add_argument('--AE_LOSS_ALPHA', type=float, default=2)\n",
    "parser.add_argument('--D_TRAIN_REPEAT', type=int, default=5)\n",
    "\n",
    "# Training settings\n",
    "parser.add_argument('--DATASET', type=str, default='CelebA', choices=['CelebA'])\n",
    "parser.add_argument('--NUM_EPOCHS', type=int, default=100)\n",
    "parser.add_argument('--NUM_EPOCHS_DECAY', type=int, default=10)\n",
    "parser.add_argument('--NUM_ITERS', type=int, default=200000)\n",
    "parser.add_argument('--NUM_ITERS_DECAY', type=int, default=100000)\n",
    "parser.add_argument('--BATCH_SIZE', type=int, default=32)\n",
    "parser.add_argument('--BETA1', type=float, default=0.5)\n",
    "parser.add_argument('--BETA2', type=float, default=0.9)\n",
    "parser.add_argument('--PRETRAINED_MODEL', type=str, default=None)\n",
    "parser.add_argument('--SPATIAL_DISCOUNTING_GAMMA', type=float, default=0.9)\n",
    "\n",
    "# Test settings\n",
    "parser.add_argument('--TEST_MODEL', type=str, default='20_1000')\n",
    "\n",
    "# Misc\n",
    "parser.add_argument('--MODE', type=str, default='train', choices=['train', 'test'])\n",
    "parser.add_argument('--USE_TENSORBOARD', type=str2bool, default=False)\n",
    "\n",
    "# Path\n",
    "parser.add_argument('--IMAGE_PATH', type=str, default='./data/CelebA/images')\n",
    "parser.add_argument('--METADATA_PATH', type=str, default='./data/list_attr_celeba.txt')\n",
    "parser.add_argument('--LOG_PATH', type=str, default='logs')\n",
    "parser.add_argument('--MODEL_SAVE_PATH', type=str, default='models')\n",
    "parser.add_argument('--SAMPLE_PATH', type=str, default='samples')\n",
    "\n",
    "# Step size\n",
    "parser.add_argument('--PRINT_EVERY', type=int, default=400)\n",
    "parser.add_argument('--SAMPLE_STEP', type=int, default=400)\n",
    "parser.add_argument('--MODEL_SAVE_STEP', type=int, default=400)\n",
    "\n",
    "# etc\n",
    "parser.add_argument('--IMG_SHAPE', type=list, default=[image_size,image_size,3])\n",
    "parser.add_argument('--MASK_HEIGHT', type=int, default=mask_size)\n",
    "parser.add_argument('--MASK_WIDTH', type=int, default=mask_size)\n",
    "parser.add_argument('--VERTICAL_MARGIN', type=int, default=0)\n",
    "parser.add_argument('--HORIZONTAL_MARGIN', type=int, default=0)\n",
    "parser.add_argument('--MAX_DELTA_HEIGHT', type=int, default=int(image_size/8))\n",
    "parser.add_argument('--MAX_DELTA_WIDTH', type=int, default=int(image_size/8))\n",
    "args = parser.parse_args(args=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data_loader.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import random\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "class CelebDataset(Dataset):\n",
    "    def __init__(self, image_path, metadata_path, transform, mode, crop_size):\n",
    "        self.image_path = image_path\n",
    "        self.transform = transform\n",
    "        self.mode = mode\n",
    "        self.lines = open(metadata_path, 'r').readlines()\n",
    "        self.num_data = int(self.lines[0])\n",
    "        self.crop_size = crop_size\n",
    "\n",
    "        print ('Start preprocessing dataset..!')\n",
    "        random.seed(1234)\n",
    "        self.preprocess()\n",
    "        print ('Finished preprocessing dataset..!')\n",
    "\n",
    "        if self.mode == 'train':\n",
    "            self.num_data = len(self.train_filenames)\n",
    "        elif self.mode == 'test':\n",
    "            self.num_data = len(self.test_filenames)\n",
    "\n",
    "    def preprocess(self):\n",
    "        self.train_filenames = []\n",
    "        self.test_filenames = []\n",
    "\n",
    "        lines = self.lines[2:]\n",
    "        random.shuffle(lines)   # random shuffling\n",
    "        for i, line in enumerate(lines):\n",
    "\n",
    "            splits = line.split()\n",
    "            filename = splits[0]\n",
    "\n",
    "            if (i+1) < 20000:\n",
    "                self.test_filenames.append(filename)\n",
    "            else:\n",
    "                self.train_filenames.append(filename)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.mode == 'train':\n",
    "            image = Image.open(os.path.join(self.image_path, self.train_filenames[index]))\n",
    "        elif self.mode in ['test']:\n",
    "            image = Image.open(os.path.join(self.image_path, self.test_filenames[index]))\n",
    "        # self.check_size(image, index)\n",
    "        return self.transform(image)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_data\n",
    "\n",
    "\n",
    "def get_loader(image_path, metadata_path, crop_size, image_size, batch_size, dataset='CelebA', mode='train'):\n",
    "    \"\"\"Build and return data loader.\"\"\"\n",
    "\n",
    "    if mode == 'train':\n",
    "        transform = transforms.Compose([\n",
    "            transforms.CenterCrop(crop_size),\n",
    "            transforms.Resize(image_size, interpolation=Image.ANTIALIAS),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            # transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "            ])\n",
    "    else:\n",
    "        transform = transforms.Compose([\n",
    "            transforms.CenterCrop(crop_size),\n",
    "            transforms.Scale(image_size, interpolation=Image.ANTIALIAS),\n",
    "            transforms.ToTensor(),\n",
    "            # transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "            ])\n",
    "\n",
    "    if dataset == 'CelebA':\n",
    "        dataset = CelebDataset(image_path, metadata_path, transform, mode, crop_size)\n",
    "\n",
    "    shuffle = False\n",
    "    if mode == 'train':\n",
    "        shuffle = True\n",
    "\n",
    "    data_loader = DataLoader(dataset=dataset,\n",
    "                             batch_size=batch_size,\n",
    "                             shuffle=shuffle)\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from model_module import *\n",
    "from torch.autograd import Variable\n",
    "from util import *\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, first_dim=32):\n",
    "        super(Generator,self).__init__()\n",
    "        self.stage_1 = CoarseNet(5, first_dim)\n",
    "        self.stage_2 = RefinementNet(5, first_dim)\n",
    "\n",
    "    def forward(self, masked_img, mask): # mask : 1 x 1 x H x W\n",
    "        # border, maybe\n",
    "        mask = mask.expand(masked_img.size(0),1,masked_img.size(2),masked_img.size(3))\n",
    "        ones = to_var(torch.ones(mask.size()))\n",
    "\n",
    "        # stage1\n",
    "        stage1_input = torch.cat([masked_img, ones, ones*mask], dim=1)\n",
    "        stage1_output, resized_mask = self.stage_1(stage1_input, mask[0].unsqueeze(0))\n",
    "\n",
    "        # stage2\n",
    "        new_masked_img = stage1_output*mask + masked_img.clone()*(1.-mask)\n",
    "        stage2_input = torch.cat([new_masked_img, ones, ones*mask], dim=1)\n",
    "        stage2_output, offset_flow = self.stage_2(stage2_input, resized_mask[0].unsqueeze(0))\n",
    "\n",
    "        return stage1_output, stage2_output, offset_flow\n",
    "\n",
    "\n",
    "class CoarseNet(nn.Module):\n",
    "    '''\n",
    "    # input: B x 5 x W x H\n",
    "    # after down: B x 128(32*4) x W/4 x H/4\n",
    "    # after atrous: same with the output size of the down module\n",
    "    # after up : same with the input size\n",
    "    '''\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(CoarseNet,self).__init__()\n",
    "        self.down = Down_Module(in_ch, out_ch)\n",
    "        self.atrous = Dilation_Module(out_ch*4, out_ch*4)\n",
    "        self.up = Up_Module(out_ch*4, 3)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x = self.down(x)\n",
    "        resized_mask = down_sample(mask, scale_factor=0.25, mode='nearest')\n",
    "        x = self.atrous(x)\n",
    "        x = self.up(x)\n",
    "        return x, resized_mask\n",
    "\n",
    "\n",
    "class RefinementNet(nn.Module):\n",
    "    '''\n",
    "    # input: B x 5 x W x H\n",
    "    # after down: B x 128(32*4) x W/4 x H/4\n",
    "    # after atrous: same with the output size of the down module\n",
    "    # after up : same with the input size\n",
    "    '''\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(RefinementNet,self).__init__()\n",
    "        self.down_conv_branch = Down_Module(in_ch, out_ch)\n",
    "        self.down_attn_branch = Down_Module(in_ch, out_ch, activation=nn.ReLU())\n",
    "        self.atrous = Dilation_Module(out_ch*4, out_ch*4)\n",
    "        self.CAttn = Contextual_Attention_Module(out_ch*4, out_ch*4)\n",
    "        self.up = Up_Module(out_ch*8, 3, isRefine=True)\n",
    "\n",
    "    def forward(self, x, resized_mask):\n",
    "        # conv branch\n",
    "        conv_x = self.down_conv_branch(x)\n",
    "        conv_x = self.atrous(conv_x)\n",
    "        \n",
    "        # attention branch\n",
    "        attn_x = self.down_attn_branch(x)\n",
    "        attn_x, offset_flow = self.CAttn(attn_x, attn_x, mask=resized_mask) # attn_x => B x 128(32*4) x W/4 x H/4\n",
    "\n",
    "        # concat two branches\n",
    "        deconv_x = torch.cat([conv_x, attn_x], dim=1) # deconv_x => B x 256 x W/4 x H/4\n",
    "        x = self.up(deconv_x)\n",
    "\n",
    "        return x, offset_flow\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, first_dim=64):\n",
    "        super(Discriminator,self).__init__()\n",
    "        self.global_discriminator = Flatten_Module(3, first_dim, False) \n",
    "        self.local_discriminator = Flatten_Module(3, first_dim, True)\n",
    "\n",
    "    def forward(self, global_x, local_x):\n",
    "        global_y = self.global_discriminator(global_x)\n",
    "        local_y = self.local_discriminator(local_x)\n",
    "        return global_y, local_y # B x 256*(256 or 512)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model_module.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from util import *\n",
    "\n",
    "\n",
    "class Conv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, K=3, S=1, P=1, D=1, activation=nn.ELU()):\n",
    "        super(Conv, self).__init__()\n",
    "        if activation:\n",
    "            self.conv = nn.Sequential(\n",
    "                nn.Conv2d(in_ch, out_ch, kernel_size=K, stride=S, padding=P, dilation=D),\n",
    "                activation\n",
    "            )\n",
    "        else:\n",
    "            self.conv = nn.Sequential(\n",
    "                nn.Conv2d(in_ch, out_ch, kernel_size=K, stride=S, padding=P, dilation=D)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# conv 1~6\n",
    "class Down_Module(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, activation=nn.ELU()):\n",
    "        super(Down_Module, self).__init__()\n",
    "        layers = []\n",
    "        layers.append(Conv(in_ch, out_ch, K=5))\n",
    "\n",
    "        curr_dim = out_ch\n",
    "        for i in range(2):\n",
    "            layers.append(Conv(curr_dim, curr_dim*2, K=3, S=2))\n",
    "            layers.append(Conv(curr_dim*2, curr_dim*2))\n",
    "            curr_dim *= 2\n",
    "        \n",
    "        layers.append(Conv(curr_dim, curr_dim, activation=activation))\n",
    "        self.out = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.out(x)\n",
    "\n",
    "\n",
    "# conv 7~10\n",
    "class Dilation_Module(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(Dilation_Module, self).__init__()\n",
    "        layers = []\n",
    "        dilation = 1\n",
    "        for i in range(4):\n",
    "            dilation *= 2\n",
    "            layers.append(Conv(in_ch, out_ch, D=dilation, P=dilation))\n",
    "        self.out = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.out(x)\n",
    "\n",
    "\n",
    "# conv 11~17\n",
    "class Up_Module(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, isRefine=False):\n",
    "        super(Up_Module, self).__init__()\n",
    "        layers = []\n",
    "        curr_dim = in_ch\n",
    "        if isRefine:\n",
    "            layers.append(Conv(curr_dim, curr_dim//2))\n",
    "            curr_dim //= 2\n",
    "        else:\n",
    "            layers.append(Conv(curr_dim, curr_dim))\n",
    "        \n",
    "        # conv 12~15\n",
    "        for i in range(2):\n",
    "            layers.append(Conv(curr_dim, curr_dim))\n",
    "            layers.append(nn.Upsample(scale_factor=2, mode='nearest'))\n",
    "            layers.append(Conv(curr_dim, curr_dim//2))\n",
    "            curr_dim //= 2\n",
    "\n",
    "        layers.append(Conv(curr_dim, curr_dim//2))\n",
    "        layers.append(Conv(curr_dim//2, out_ch, activation=0))\n",
    "\n",
    "        self.out = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.out(x)\n",
    "        return torch.clamp(output, min=-1., max=1.)\n",
    "\n",
    "\n",
    "class Flatten_Module(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, isLocal=True):\n",
    "        super(Flatten_Module, self).__init__()\n",
    "        layers = []\n",
    "        layers.append(Conv(in_ch, out_ch, K=5, S=2, P=2, activation=nn.LeakyReLU()))\n",
    "        curr_dim = out_ch\n",
    "\n",
    "        for i in range(2):\n",
    "            layers.append(Conv(curr_dim, curr_dim*2, K=5, S=2, P=2, activation=nn.LeakyReLU()))\n",
    "            curr_dim *= 2\n",
    "        \n",
    "        if isLocal:\n",
    "            layers.append(Conv(curr_dim, curr_dim*2, K=5, S=2, P=2, activation=nn.LeakyReLU()))\n",
    "        else:\n",
    "            layers.append(Conv(curr_dim, curr_dim, K=5, S=2, P=2, activation=nn.LeakyReLU()))\n",
    "\n",
    "        self.out = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.out(x)\n",
    "        return x.view(x.size(0),-1) # 2B x 256*(256 or 512); front 256:16*16\n",
    "\n",
    "\n",
    "# pmconv 9~10\n",
    "class Contextual_Attention_Module(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, rate=2, stride=1):\n",
    "        super(Contextual_Attention_Module, self).__init__()\n",
    "        self.rate = rate\n",
    "        self.padding = nn.ZeroPad2d(1)\n",
    "        self.up_sample = nn.Upsample(scale_factor=self.rate, mode='nearest')\n",
    "        layers = []\n",
    "        for i in range(2):\n",
    "            layers.append(Conv(in_ch, out_ch))\n",
    "        self.out = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, f, b, mask=None, ksize=3, stride=1, \n",
    "                fuse_k=3, softmax_scale=10., training=True, fuse=True):\n",
    "\n",
    "        \"\"\" Contextual attention layer implementation.\n",
    "\n",
    "        Contextual attention is first introduced in publication:\n",
    "            Generative Image Inpainting with Contextual Attention, Yu et al.\n",
    "\n",
    "        Args:\n",
    "            f: Input feature to match (foreground).\n",
    "            b: Input feature for match (background).\n",
    "            mask: Input mask for b, indicating patches not available.\n",
    "            ksize: Kernel size for contextual attention.\n",
    "            stride: Stride for extracting patches from b.\n",
    "            rate: Dilation for matching.\n",
    "            softmax_scale: Scaled softmax for attention.\n",
    "            training: Indicating if current graph is training or inference.\n",
    "\n",
    "        Returns:\n",
    "            tf.Tensor: output\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # get shapes\n",
    "        raw_fs = f.size() # B x 128 x 64 x 64\n",
    "        raw_int_fs = list(f.size())\n",
    "        raw_int_bs = list(b.size())\n",
    "\n",
    "        # extract patches from background with stride and rate\n",
    "        kernel = 2*self.rate\n",
    "        raw_w = self.extract_patches(b, kernel=kernel, stride=self.rate)\n",
    "        raw_w = raw_w.contiguous().view(raw_int_bs[0], -1, raw_int_bs[1], kernel, kernel) # B*HW*C*K*K (B, 32*32, 128, 4, 4)\n",
    "\n",
    "        # downscaling foreground option: downscaling both foreground and\n",
    "        # background for matching and use original background for reconstruction.\n",
    "        f = down_sample(f, scale_factor=1/self.rate, mode='nearest')\n",
    "        b = down_sample(b, scale_factor=1/self.rate, mode='nearest')\n",
    "        fs = f.size() # B x 128 x 32 x 32\n",
    "        int_fs = list(f.size())\n",
    "        f_groups = torch.split(f, 1, dim=0) # Split tensors by batch dimension; tuple is returned\n",
    "\n",
    "        # from b(B*H*W*C) to w(b*k*k*c*h*w)\n",
    "        bs = b.size() # B x 128 x 32 x 32\n",
    "        int_bs = list(b.size())\n",
    "        w = self.extract_patches(b)\n",
    "\n",
    "        w = w.contiguous().view(int_fs[0], -1, int_fs[1], ksize, ksize) # B*HW*C*K*K (B, 32*32, 128, 3, 3)\n",
    "\n",
    "        # process mask\n",
    "        if mask is not None:\n",
    "            mask = down_sample(mask, scale_factor=1./self.rate, mode='nearest')\n",
    "        else:\n",
    "            mask = torch.zeros([1, 1, bs[2], bs[3]])\n",
    "\n",
    "        m = self.extract_patches(mask)\n",
    "\n",
    "        m = m.contiguous().view(1, 1, -1, ksize, ksize)  # B*C*HW*K*K\n",
    "        m = m[0] # (1, 32*32, 3, 3)\n",
    "        m = reduce_mean(m) # smoothing, maybe\n",
    "        mm = m.eq(0.).float() # (1, 32*32, 1, 1)       \n",
    "        \n",
    "        w_groups = torch.split(w, 1, dim=0) # Split tensors by batch dimension; tuple is returned\n",
    "        raw_w_groups = torch.split(raw_w, 1, dim=0) # Split tensors by batch dimension; tuple is returned\n",
    "        y = []\n",
    "        offsets = []\n",
    "        k = fuse_k\n",
    "        scale = softmax_scale\n",
    "        fuse_weight = Variable(torch.eye(k).view(1, 1, k, k)).cuda() # 1 x 1 x K x K\n",
    "\n",
    "        for xi, wi, raw_wi in zip(f_groups, w_groups, raw_w_groups):\n",
    "            '''\n",
    "            O => output channel as a conv filter\n",
    "            I => input channel as a conv filter\n",
    "            xi : separated tensor along batch dimension of front; (B=1, C=128, H=32, W=32)\n",
    "            wi : separated patch tensor along batch dimension of back; (B=1, O=32*32, I=128, KH=3, KW=3)\n",
    "            raw_wi : separated tensor along batch dimension of back; (B=1, I=32*32, O=128, KH=4, KW=4)\n",
    "            '''\n",
    "            # conv for compare\n",
    "            wi = wi[0]\n",
    "            escape_NaN = Variable(torch.FloatTensor([1e-4])).cuda()\n",
    "            wi_normed = wi / torch.max(l2_norm(wi), escape_NaN)\n",
    "            yi = F.conv2d(xi, wi_normed, stride=1, padding=1) # yi => (B=1, C=32*32, H=32, W=32)\n",
    "\n",
    "            # conv implementation for fuse scores to encourage large patches\n",
    "            if fuse:\n",
    "                yi = yi.view(1, 1, fs[2]*fs[3], bs[2]*bs[3]) # make all of depth to spatial resolution, (B=1, I=1, H=32*32, W=32*32)\n",
    "                yi = F.conv2d(yi, fuse_weight, stride=1, padding=1) # (B=1, C=1, H=32*32, W=32*32)\n",
    "\n",
    "                yi = yi.contiguous().view(1, fs[2], fs[3], bs[2], bs[3]) # (B=1, 32, 32, 32, 32)\n",
    "                yi = yi.permute(0, 2, 1, 4, 3)\n",
    "                yi = yi.contiguous().view(1, 1, fs[2]*fs[3], bs[2]*bs[3])\n",
    "                \n",
    "                yi = F.conv2d(yi, fuse_weight, stride=1, padding=1)\n",
    "                yi = yi.contiguous().view(1, fs[3], fs[2], bs[3], bs[2])\n",
    "                yi = yi.permute(0, 2, 1, 4, 3)\n",
    "\n",
    "            yi = yi.contiguous().view(1, bs[2]*bs[3], fs[2], fs[3]) # (B=1, C=32*32, H=32, W=32)\n",
    "\n",
    "            # softmax to match\n",
    "            yi = yi * mm  # mm => (1, 32*32, 1, 1)\n",
    "            yi = F.softmax(yi*scale, dim=1)\n",
    "            yi = yi * mm  # mask\n",
    "\n",
    "            _, offset = torch.max(yi, dim=1) # argmax; index\n",
    "            division = torch.div(offset, fs[3]).long()\n",
    "            offset = torch.stack([division, torch.div(offset, fs[3])-division], dim=-1)\n",
    "\n",
    "            # deconv for patch pasting\n",
    "            # 3.1 paste center\n",
    "            wi_center = raw_wi[0]\n",
    "            yi = F.conv_transpose2d(yi, wi_center, stride=self.rate, padding=1) / 4. # (B=1, C=128, H=64, W=64)\n",
    "            y.append(yi)\n",
    "            offsets.append(offset)\n",
    "\n",
    "        y = torch.cat(y, dim=0) # back to the mini-batch\n",
    "        y.contiguous().view(raw_int_fs)\n",
    "        offsets = torch.cat(offsets, dim=0)\n",
    "        offsets = offsets.view([int_bs[0]] + [2] + int_bs[2:])\n",
    "\n",
    "        # case1: visualize optical flow: minus current position\n",
    "        h_add = Variable(torch.arange(0,float(bs[2]))).cuda().view([1, 1, bs[2], 1])\n",
    "        h_add = h_add.expand(bs[0], 1, bs[2], bs[3])\n",
    "        w_add = Variable(torch.arange(0,float(bs[3]))).cuda().view([1, 1, 1, bs[3]])\n",
    "        w_add = w_add.expand(bs[0], 1, bs[2], bs[3])\n",
    "\n",
    "        offsets = offsets - torch.cat([h_add, w_add], dim=1).long()\n",
    "\n",
    "        # to flow image\n",
    "        flow = torch.from_numpy(flow_to_image(offsets.permute(0,2,3,1).cpu().data.numpy()))\n",
    "        flow = flow.permute(0,3,1,2)\n",
    "\n",
    "        # # case2: visualize which pixels are attended\n",
    "        # flow = torch.from_numpy(highlight_flow((offsets * mask.int()).numpy()))\n",
    "        if self.rate != 1:\n",
    "            flow = self.up_sample(flow)\n",
    "        return self.out(y), flow\n",
    "\n",
    "    # padding1(16 x 128 x 64 x 64) => (16 x 128 x 64 x 64 x 3 x 3)\n",
    "    def extract_patches(self, x, kernel=3, stride=1):\n",
    "        x = self.padding(x)\n",
    "        all_patches = x.unfold(2, kernel, stride).unfold(3, kernel, stride)\n",
    "        return all_patches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "cuda runtime error (35) : CUDA driver version is insufficient for CUDA runtime version at ..\\torch\\csrc\\cuda\\Module.cpp:33",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-010e42b5fb3d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    285\u001b[0m         \u001b[0mrun\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    286\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 287\u001b[1;33m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-11-010e42b5fb3d>\u001b[0m in \u001b[0;36mmain\u001b[1;34m(_)\u001b[0m\n\u001b[0;32m    276\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 278\u001b[1;33m     \u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_device\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGPU\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    279\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Running on GPU : \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGPU\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    280\u001b[0m     \u001b[0mrun\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\udacity\\lib\\site-packages\\torch\\cuda\\__init__.py\u001b[0m in \u001b[0;36mset_device\u001b[1;34m(device)\u001b[0m\n\u001b[0;32m    263\u001b[0m     \u001b[0mdevice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_device_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    264\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdevice\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 265\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cuda_setDevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    266\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: cuda runtime error (35) : CUDA driver version is insufficient for CUDA runtime version at ..\\torch\\csrc\\cuda\\Module.cpp:33"
     ]
    }
   ],
   "source": [
    "import torch as nn\n",
    "import torch.optim as optim\n",
    "from torch import cuda\n",
    "from config import *\n",
    "from model import *\n",
    "from data_loader import *\n",
    "from util import *\n",
    "import time\n",
    "import datetime\n",
    "import os\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "class Run(object):\n",
    "    def __init__(self, args):\n",
    "        # Data loader\n",
    "        if args.DATASET == 'CelebA':\n",
    "            self.data_loader = get_loader(args.IMAGE_PATH, args.METADATA_PATH, \n",
    "                                        args.CROP_SIZE, args.IMG_SIZE, \n",
    "                                        args.BATCH_SIZE, args.DATASET, args.MODE)\n",
    "\n",
    "        # Model hyper-parameters\n",
    "        self.image_shape = args.IMG_SHAPE\n",
    "\n",
    "        # Hyper-parameteres\n",
    "        self.stage1_lambda_l1 = args.COARSE_L1_ALPHA\n",
    "        self.global_wgan_loss_alpha = args.GLOBAL_WGAN_LOSS_ALPHA\n",
    "        self.wgan_gp_lambda = args.WGAN_GP_LAMBDA\n",
    "        self.gan_loss_alpha = args.GAN_LOSS_ALPHA\n",
    "        self.l1_loss_alpha = args.L1_LOSS_ALPHA\n",
    "        self.ae_loss_alpha = args.AE_LOSS_ALPHA\n",
    "        self.g_lr = args.G_LR\n",
    "        self.d_lr = args.D_LR\n",
    "        self.beta1 = args.BETA1\n",
    "        self.beta2 = args.BETA2\n",
    "\n",
    "        # Training settings\n",
    "        self.dataset = args.DATASET\n",
    "        self.num_epochs = args.NUM_EPOCHS\n",
    "        self.num_epochs_decay = args.NUM_EPOCHS_DECAY\n",
    "        self.num_iters = args.NUM_ITERS\n",
    "        self.num_iters_decay = args.NUM_ITERS_DECAY\n",
    "        self.batch_size = args.BATCH_SIZE\n",
    "        self.use_tensorboard = args.USE_TENSORBOARD\n",
    "        self.pretrained_model = args.PRETRAINED_MODEL\n",
    "        self.d_train_repeat = args.D_TRAIN_REPEAT\n",
    "\n",
    "        # Test settings\n",
    "        self.test_model = args.TEST_MODEL\n",
    "\n",
    "        # Path\n",
    "        self.sample_path = args.SAMPLE_PATH\n",
    "        self.model_save_path = args.MODEL_SAVE_PATH\n",
    "\n",
    "        # Step size\n",
    "        self.print_every = args.PRINT_EVERY\n",
    "        self.sample_step = args.SAMPLE_STEP\n",
    "        self.model_save_step = args.MODEL_SAVE_STEP\n",
    "\n",
    "        # etc\n",
    "        self.make_dir()\n",
    "        self.init_network(args)\n",
    "        self.loss = {}\n",
    "\n",
    "        if self.pretrained_model:\n",
    "            self.load_pretrained_model()\n",
    "\n",
    "    def make_dir(self):\n",
    "        if not os.path.exists(self.model_save_path):\n",
    "            os.makedirs(self.model_save_path)\n",
    "        if not os.path.exists(self.sample_path):\n",
    "            os.makedirs(self.sample_path)\n",
    "\n",
    "    def init_network(self, args):\n",
    "\n",
    "        # Models\n",
    "        self.G = Generator()\n",
    "        self.D = Discriminator()\n",
    "\n",
    "        # Optimizers\n",
    "        self.g_optimizer = optim.Adam(self.G.parameters(), self.g_lr, [self.beta1, self.beta2])\n",
    "        self.d_optimizer = optim.Adam(self.D.parameters(), self.d_lr, [self.beta1, self.beta2])\n",
    "\n",
    "        # Loss\n",
    "        self.L1 = Discounted_L1(args)\n",
    "        self.torch_L1 = nn.L1Loss()\n",
    "\n",
    "        # etc.\n",
    "        self.util = Util(args)\n",
    "\n",
    "        # Print networks\n",
    "        # self.util.print_network(self.G, 'G')\n",
    "        # self.util.print_network(self.D, 'D')\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            self.G = self.G.cuda()\n",
    "            self.D = self.D.cuda()\n",
    "            self.L1 = self.L1.cuda()\n",
    "            self.torch_L1 = nn.L1Loss().cuda()\n",
    "\n",
    "    def load_pretrained_model(self):\n",
    "        self.G.load_state_dict(torch.load(os.path.join(\n",
    "            self.model_save_path, 'G_{}_L1_{}.pth'.format(self.pretrained_model, self.l1_loss_alpha))))\n",
    "        self.D.load_state_dict(torch.load(os.path.join(\n",
    "            self.model_save_path, 'D_{}_L1_{}.pth'.format(self.pretrained_model, self.l1_loss_alpha))))\n",
    "\n",
    "        print('loaded trained models (step: {})..!'.format(self.pretrained_model))\n",
    "\n",
    "    def train(self):   \n",
    "\n",
    "        # The number of iterations per epoch\n",
    "        iters_per_epoch = len(self.data_loader)\n",
    "\n",
    "        # lr cache for decaying\n",
    "        g_lr = self.g_lr\n",
    "        d_lr = self.d_lr\n",
    "\n",
    "        if self.pretrained_model:\n",
    "            start = int(self.pretrained_model.split('_')[0])\n",
    "        else:\n",
    "            start = 0\n",
    "\n",
    "        start_time = time.time()\n",
    "        self.G.train()\n",
    "        self.D.train()\n",
    "        for epoch in range(start, self.num_epochs):\n",
    "            for batch, real_image in enumerate(self.data_loader): # real_image : B x 3 x H x W\n",
    "                \n",
    "                batch_size = real_image.size(0)\n",
    "                real_image = 2.*real_image - 1. # [-1,1]\n",
    "                \n",
    "                # one bbox for each batch, ( top, left, maxH, maxW )\n",
    "                # W and H will be reduced at the function bbox2mask\n",
    "                bbox = self.util.random_bbox()\n",
    "\n",
    "                binary_mask = self.util.bbox2mask(bbox)\n",
    "                inverse_mask = 1.- binary_mask\n",
    "                masked_image = real_image.clone()*inverse_mask\n",
    "\n",
    "                binary_mask = to_var(binary_mask)\n",
    "                inverse_mask = to_var(inverse_mask)\n",
    "                masked_image = to_var(masked_image)\n",
    "                real_image = to_var(real_image)\n",
    "\n",
    "                stage_1, stage_2, offset_flow = self.G(masked_image, binary_mask)\n",
    "                \n",
    "                fake_image = stage_2*binary_mask + masked_image*inverse_mask # mask_location: generated, around_mask: ground_truth\n",
    "\n",
    "                real_patch = self.util.local_patch(real_image, bbox)\n",
    "                stage_1_patch = self.util.local_patch(stage_1, bbox)\n",
    "                stage_2_patch = self.util.local_patch(stage_2, bbox)\n",
    "                mask_patch = self.util.local_patch(binary_mask, bbox)\n",
    "                fake_patch = self.util.local_patch(fake_image, bbox)\n",
    "\n",
    "                l1_alpha = self.stage1_lambda_l1\n",
    "                self.loss['recon'] = l1_alpha * self.L1(stage_1_patch, real_patch) # Coarse Network reconstruction loss\n",
    "                self.loss['recon'] = self.loss['recon'] + self.L1(stage_2_patch, real_patch) # Refinement Network reconstruction loss\n",
    "                \n",
    "                self.loss['ae_loss'] = l1_alpha * self.torch_L1(stage_1*inverse_mask, real_image*inverse_mask) # recon loss except mask\n",
    "                self.loss['ae_loss'] = self.loss['ae_loss'] + self.torch_L1(stage_2*inverse_mask, real_image*inverse_mask) # recon loss except mask\n",
    "                self.loss['ae_loss'] = self.loss['ae_loss'] / torch.mean(torch.mean(inverse_mask, dim=3), dim=2) # 1 x 1 tensor\n",
    "\n",
    "                if (batch+1) % self.d_train_repeat == 0:\n",
    "                    global_real_fake_image = torch.cat([real_image, fake_image], dim=0)\n",
    "                    local_real_fake_image = torch.cat([real_patch, fake_patch], dim=0)\n",
    "                else:\n",
    "                    global_real_fake_image = torch.cat([real_image, fake_image.clone()], dim=0)\n",
    "                    local_real_fake_image = torch.cat([real_patch, fake_patch.clone()], dim=0)\n",
    "\n",
    "                global_real_fake_vector, local_real_fake_vector = self.D(global_real_fake_image, local_real_fake_image)\n",
    "\n",
    "                global_real_vector, global_fake_vector = torch.split(global_real_fake_vector, batch_size, dim=0)\n",
    "                local_real_vector, local_fake_vector = torch.split(local_real_fake_vector, batch_size, dim=0)\n",
    "\n",
    "                global_G_loss, global_D_loss = self.wgan_loss(global_real_vector, global_fake_vector)\n",
    "                local_G_loss, local_D_loss = self.wgan_loss(local_real_vector, local_fake_vector)\n",
    "\n",
    "                self.loss['g_loss'] = self.global_wgan_loss_alpha * (global_G_loss + local_G_loss)\n",
    "                self.loss['d_loss'] = global_D_loss + local_D_loss\n",
    "\n",
    "                if (batch+1) % self.d_train_repeat == 0:\n",
    "                    # gradient penalty\n",
    "                    global_interpolate = self.random_interpolates(real_image, fake_image) \n",
    "                    local_interpolate = self.random_interpolates(real_patch, fake_patch)\n",
    "                else:\n",
    "                    global_interpolate = self.random_interpolates(real_image, fake_image.clone()) \n",
    "                    local_interpolate = self.random_interpolates(real_patch, fake_patch.clone())\n",
    "\n",
    "                global_gp_vector, local_gp_vector = self.D(global_interpolate, local_interpolate)\n",
    "\n",
    "                global_penalty = self.gradient_penalty(global_interpolate, global_gp_vector, mask=binary_mask)\n",
    "                local_penalty = self.gradient_penalty(local_interpolate, local_gp_vector, mask=mask_patch)\n",
    "\n",
    "                self.loss['gp_loss'] = self.wgan_gp_lambda * (local_penalty + global_penalty)\n",
    "                self.loss['d_loss'] = self.loss['d_loss'] + self.loss['gp_loss']\n",
    "\n",
    "                if (batch+1) % self.d_train_repeat == 0:             \n",
    "                    self.loss['g_loss'] = self.gan_loss_alpha * self.loss['g_loss']\n",
    "                    self.loss['g_loss'] = self.loss['g_loss'] + self.l1_loss_alpha * self.loss['recon'] + self.ae_loss_alpha * self.loss['ae_loss']\n",
    "                    self.backprop(D=True,G=True)\n",
    "\n",
    "                else:                  \n",
    "                    self.loss['g_loss'] = to_var(torch.FloatTensor([0]))\n",
    "                    self.backprop(D=True,G=False)\n",
    "\n",
    "\n",
    "                if batch % self.print_every == 0:\n",
    "                    elapsed = time.time() - start_time\n",
    "                    elapsed = str(datetime.timedelta(seconds=elapsed))\n",
    "\n",
    "                    print('=====================================================')\n",
    "                    print(\"Elapsed [{}], Epoch [{}/{}], Iter [{}/{}]\".format(\n",
    "                        elapsed, epoch+1, self.num_epochs, batch+1, iters_per_epoch))\n",
    "                    print('=====================================================')\n",
    "                    print('reconstruction loss: ', self.loss['recon'].data[0])\n",
    "                    print('ae loss: ', self.loss['ae_loss'].data[0][0])\n",
    "                    print('g loss: ', self.loss['g_loss'].data[0])\n",
    "                    print('d loss: ', self.loss['d_loss'].data[0])\n",
    "                    show_image(real_image, (masked_image+binary_mask), stage_1, stage_2, fake_image, offset_flow)\n",
    "\n",
    "                # Save model checkpoints\n",
    "                if batch % self.model_save_step == 0:\n",
    "                    torch.save(self.G.state_dict(),\n",
    "                        os.path.join(self.model_save_path, 'G_{}_L1_{}.pth'.format(epoch+1, self.l1_loss_alpha)))\n",
    "                    torch.save(self.D.state_dict(),\n",
    "                        os.path.join(self.model_save_path, 'D_{}_L1_{}.pth'.format(epoch+1, self.l1_loss_alpha)))\n",
    "\n",
    "                # Save sample image\n",
    "                if batch % self.sample_step == 0:\n",
    "                    save_image(self.denorm(fake_image.clone().data.cpu()),\n",
    "                        os.path.join(self.sample_path, '{}_{}_fake.png'.format(epoch+1, batch+1)),nrow=1, padding=0)\n",
    "                    print('Translated images and saved into {}..!'.format(self.sample_path))\n",
    "\n",
    "    def backprop(self, D=True, G=True):\n",
    "        if D:\n",
    "            self.d_optimizer.zero_grad()\n",
    "            self.loss['d_loss'].backward(retain_graph=G)\n",
    "            self.d_optimizer.step()\n",
    "        if G:\n",
    "            self.g_optimizer.zero_grad()\n",
    "            self.loss['g_loss'].backward()\n",
    "            self.g_optimizer.step()\n",
    "\n",
    "    def wgan_loss(self, real, fake):\n",
    "        diff = fake - real\n",
    "        d_loss = torch.mean(diff)\n",
    "        g_loss = -torch.mean(fake)\n",
    "        return g_loss, d_loss\n",
    "\n",
    "    def random_interpolates(self, real, fake, alpha=None):\n",
    "        shape = list(real.size())\n",
    "        real = real.contiguous().view(shape[0], -1, 1, 1)\n",
    "        fake = fake.contiguous().view(shape[0], -1, 1, 1)\n",
    "        if alpha is None:\n",
    "            alpha = Variable(torch.rand(shape[0], 1, 1, 1)).cuda()\n",
    "        interpolates = fake + alpha*(real - fake)\n",
    "        return interpolates.view(shape)\n",
    "\n",
    "    def gradient_penalty(self, x, y, mask=None, norm=1.):\n",
    "        \"\"\"Compute gradient penalty: (L2_norm(dy/dx) - 1)**2.\"\"\"\n",
    "        weight = Variable(torch.ones(y.size())).cuda()\n",
    "        dydx = torch.autograd.grad(outputs=y,\n",
    "                                   inputs=x,\n",
    "                                   grad_outputs=weight,\n",
    "                                   retain_graph=True,\n",
    "                                   create_graph=True,\n",
    "                                   only_inputs=True)[0]\n",
    "        dydx = dydx * mask\n",
    "        dydx = dydx.view(dydx.size(0), -1)\n",
    "        dydx_l2norm = torch.sqrt(torch.sum(dydx**2, dim=1))\n",
    "        return torch.mean((dydx_l2norm-1)**2)\n",
    "\n",
    "    def denorm(self, x):\n",
    "        out = (x + 1) / 2\n",
    "        return out.clamp_(0, 1)\n",
    "\n",
    "def main(_):\n",
    "\n",
    "    cuda.set_device(args.GPU)\n",
    "    print(\"Running on GPU : \", args.GPU)\n",
    "    run = Run(args)\n",
    "\n",
    "    if args.MODE == 'train':\n",
    "        run.train()\n",
    "    else:\n",
    "        run.test()\n",
    "\n",
    "main(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "util.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.backends import cudnn\n",
    "from random import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Util(object):\n",
    "    def __init__(self,args):\n",
    "        self.args = args\n",
    "\n",
    "    def print_network(self, model, name):\n",
    "        num_params = 0\n",
    "        for p in model.parameters():\n",
    "            num_params += p.numel()\n",
    "        print(name)\n",
    "        print(model)\n",
    "        print(\"The number of parameters: {}\".format(num_params))\n",
    "\n",
    "    def random_bbox(self):\n",
    "        img_shape = self.args.IMG_SHAPE\n",
    "        img_height = img_shape[0]\n",
    "        img_width = img_shape[1]\n",
    "\n",
    "        maxt = img_height - self.args.VERTICAL_MARGIN - self.args.MASK_HEIGHT\n",
    "        maxl = img_width - self.args.HORIZONTAL_MARGIN - self.args.MASK_WIDTH\n",
    "        \n",
    "        t = randint(self.args.VERTICAL_MARGIN, maxt)\n",
    "        l = randint(self.args.HORIZONTAL_MARGIN, maxl)\n",
    "        h = self.args.MASK_HEIGHT\n",
    "        w = self.args.MASK_WIDTH\n",
    "        return (t, l, h, w)\n",
    "\n",
    "    def bbox2mask(self, bbox):\n",
    "        \"\"\"Generate mask tensor from bbox.\n",
    "\n",
    "        Args:\n",
    "            bbox: configuration tuple, (top, left, height, width)\n",
    "            config: Config should have configuration including IMG_SHAPES,\n",
    "                MAX_DELTA_HEIGHT, MAX_DELTA_WIDTH.\n",
    "\n",
    "        Returns:\n",
    "            tf.Tensor: output with shape [B, 1, H, W]\n",
    "\n",
    "        \"\"\"\n",
    "        def npmask(bbox, height, width, delta_h, delta_w):\n",
    "            mask = np.zeros((1, 1, height, width), np.float32)\n",
    "            h = np.random.randint(delta_h//2+1)\n",
    "            w = np.random.randint(delta_w//2+1)\n",
    "            mask[:, :, bbox[0]+h : bbox[0]+bbox[2]-h,\n",
    "                 bbox[1]+w : bbox[1]+bbox[3]-w] = 1.\n",
    "            return mask\n",
    "\n",
    "        img_shape = self.args.IMG_SHAPE\n",
    "        height = img_shape[0]\n",
    "        width = img_shape[1]\n",
    "\n",
    "        mask = npmask(bbox, height, width, \n",
    "                        self.args.MAX_DELTA_HEIGHT, \n",
    "                        self.args.MAX_DELTA_WIDTH)\n",
    "        \n",
    "        return torch.FloatTensor(mask)\n",
    "\n",
    "    def local_patch(self, x, bbox):\n",
    "        '''\n",
    "        bbox[0]: top\n",
    "        bbox[1]: left\n",
    "        bbox[2]: height\n",
    "        bbox[3]: width\n",
    "        '''\n",
    "        x = x[:, :, bbox[0]:bbox[0]+bbox[2], bbox[1]:bbox[1]+bbox[3]]\n",
    "        return x\n",
    "\n",
    "\n",
    "class Discounted_L1(nn.Module):\n",
    "    def __init__(self, args, size_average=True, reduce=True):\n",
    "        super(Discounted_L1, self).__init__()\n",
    "        self.reduce = reduce\n",
    "        self.discounting_mask = spatial_discounting_mask(args.MASK_WIDTH, \n",
    "                                                        args.MASK_HEIGHT, \n",
    "                                                        args.SPATIAL_DISCOUNTING_GAMMA)\n",
    "        self.size_average = size_average\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        self._assert_no_grad(target)\n",
    "        return self._pointwise_loss(lambda a, b: torch.abs(a - b), torch._C._nn.l1_loss,\n",
    "                           input, target, self.discounting_mask, self.size_average, self.reduce)\n",
    "\n",
    "    def _assert_no_grad(self, variable):\n",
    "        assert not variable.requires_grad, \\\n",
    "        \"nn criterions don't compute the gradient w.r.t. targets - please \" \\\n",
    "        \"mark these variables as volatile or not requiring gradients\"\n",
    "\n",
    "    def _pointwise_loss(self, lambd, lambd_optimized, input, target, discounting_mask, size_average=True, reduce=True):\n",
    "        if target.requires_grad:\n",
    "            d = lambd(input, target)\n",
    "            d = d * discounting_mask\n",
    "            if not reduce:\n",
    "                return d\n",
    "            return torch.mean(d) if size_average else torch.sum(d)\n",
    "        else:\n",
    "            return lambd_optimized(input, target, size_average, reduce)\n",
    "\n",
    "\n",
    "def spatial_discounting_mask(mask_width, mask_height, discounting_gamma):\n",
    "    \"\"\"Generate spatial discounting mask constant.\n",
    "\n",
    "    Spatial discounting mask is first introduced in publication:\n",
    "        Generative Image Inpainting with Contextual Attention, Yu et al.\n",
    "\n",
    "    Returns:\n",
    "        tf.Tensor: spatial discounting mask\n",
    "\n",
    "    \"\"\"\n",
    "    gamma = discounting_gamma\n",
    "    shape = [1, 1, mask_width, mask_height]\n",
    "    if True:\n",
    "        print('Use spatial discounting l1 loss.')\n",
    "        mask_values = np.ones((mask_width, mask_height))\n",
    "        for i in range(mask_width):\n",
    "            for j in range(mask_height):\n",
    "                mask_values[i, j] = max(\n",
    "                    gamma**min(i, mask_width-i),\n",
    "                    gamma**min(j, mask_height-j))\n",
    "        mask_values = np.expand_dims(mask_values, 0)\n",
    "        mask_values = np.expand_dims(mask_values, 1)\n",
    "        mask_values = mask_values\n",
    "    else:\n",
    "        mask_values = np.ones(shape)\n",
    "    # it will be extended along the batch dimension suitably\n",
    "    mask_values = torch.from_numpy(mask_values).float()\n",
    "    return to_var(mask_values)\n",
    "\n",
    "\n",
    "def down_sample(x, size=None, scale_factor=None, mode='nearest'):\n",
    "    # define size if user has specified scale_factor\n",
    "    if size is None: size = (int(scale_factor*x.size(2)), int(scale_factor*x.size(3)))\n",
    "    # create coordinates\n",
    "    h = torch.arange(0,size[0]) / (size[0]-1) * 2 - 1\n",
    "    w = torch.arange(0,size[1]) / (size[1]-1) * 2 - 1\n",
    "    # create grid\n",
    "    grid =torch.zeros(size[0],size[1],2)\n",
    "    grid[:,:,0] = w.unsqueeze(0).repeat(size[0],1)\n",
    "    grid[:,:,1] = h.unsqueeze(0).repeat(size[1],1).transpose(0,1)\n",
    "    # expand to match batch size\n",
    "    grid = grid.unsqueeze(0).repeat(x.size(0),1,1,1)\n",
    "    if x.is_cuda: grid = Variable(grid).cuda()\n",
    "    # do sampling\n",
    "    return F.grid_sample(x, grid, mode=mode)\n",
    "\n",
    "\n",
    "def reduce_mean(x):\n",
    "    for i in range(4):\n",
    "        if i==1: continue\n",
    "        x = torch.mean(x, dim=i, keepdim=True)\n",
    "    return x\n",
    "\n",
    "\n",
    "def l2_norm(x):\n",
    "    def reduce_sum(x):\n",
    "        for i in range(4):\n",
    "            if i==1: continue\n",
    "            x = torch.sum(x, dim=i, keepdim=True)\n",
    "        return x\n",
    "\n",
    "    x = x**2\n",
    "    x = reduce_sum(x)\n",
    "    return torch.sqrt(x)\n",
    "\n",
    "\n",
    "def show_image(real, masked, stage_1, stage_2, fake, offset_flow):\n",
    "    batch_size = real.shape[0]\n",
    "\n",
    "    (real, masked, stage_1, stage_2, fake, offset_flow) = (\n",
    "                                var_to_numpy(real), \n",
    "                                var_to_numpy(masked), \n",
    "                                var_to_numpy(stage_1),\n",
    "                                var_to_numpy(stage_2),\n",
    "                                var_to_numpy(fake),\n",
    "                                var_to_numpy(offset_flow)\n",
    "                              )\n",
    "    # offset_flow = (offset_flow*2).astype(int) -1\n",
    "    for x in range(batch_size):\n",
    "        if x > 5 :\n",
    "            break\n",
    "        fig, axs = plt.subplots(ncols=5, figsize=(15,3))\n",
    "        axs[0].set_title('real image')\n",
    "        axs[0].imshow(real[x])\n",
    "        axs[0].axis('off')\n",
    "\n",
    "        axs[1].set_title('masked image')\n",
    "        axs[1].imshow(masked[x])\n",
    "        axs[1].axis('off')\n",
    "\n",
    "        axs[2].set_title('stage_1 image')\n",
    "        axs[2].imshow(stage_1[x])\n",
    "        axs[2].axis('off')\n",
    "\n",
    "        axs[3].set_title('stage_2 image')\n",
    "        axs[3].imshow(stage_2[x])\n",
    "        axs[3].axis('off')\n",
    "\n",
    "        axs[4].set_title('fake_image')\n",
    "        axs[4].imshow(fake[x])\n",
    "        axs[4].axis('off')\n",
    "\n",
    "        # axs[5].set_title('C_Attn')\n",
    "        # axs[5].imshow(offset_flow[x])\n",
    "        # axs[5].axis('off')\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def var_to_numpy(obj, for_vis=True):\n",
    "    if for_vis:\n",
    "        obj = obj.permute(0,2,3,1)\n",
    "        obj = (obj+1) / 2\n",
    "    return obj.data.cpu().numpy()\n",
    "\n",
    "\n",
    "def to_var(x, volatile=False):\n",
    "    if torch.cuda.is_available():\n",
    "        x = x.cuda()\n",
    "    return Variable(x, volatile=volatile)\n",
    "\n",
    "\n",
    "def flow_to_image(flow):\n",
    "    \"\"\"Transfer flow map to image.\n",
    "    Part of code forked from flownet.\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    maxu = -999.\n",
    "    maxv = -999.\n",
    "    minu = 999.\n",
    "    minv = 999.\n",
    "    maxrad = -1\n",
    "    for i in range(flow.shape[0]):\n",
    "        u = flow[i, :, :, 0]\n",
    "        v = flow[i, :, :, 1]\n",
    "        idxunknow = (abs(u) > 1e7) | (abs(v) > 1e7)\n",
    "        u[idxunknow] = 0\n",
    "        v[idxunknow] = 0\n",
    "        maxu = max(maxu, np.max(u))\n",
    "        minu = min(minu, np.min(u))\n",
    "        maxv = max(maxv, np.max(v))\n",
    "        minv = min(minv, np.min(v))\n",
    "        rad = np.sqrt(u ** 2 + v ** 2)\n",
    "        maxrad = max(maxrad, np.max(rad))\n",
    "        u = u/(maxrad + np.finfo(float).eps)\n",
    "        v = v/(maxrad + np.finfo(float).eps)\n",
    "        img = compute_color(u, v)\n",
    "        out.append(img)\n",
    "    return np.float32(np.uint8(out))\n",
    "\n",
    "\n",
    "def highlight_flow(flow):\n",
    "    \"\"\"Convert flow into middlebury color code image.\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    s = flow.shape\n",
    "    for i in range(flow.shape[0]):\n",
    "        img = np.ones((s[1], s[2], 3)) * 144.\n",
    "        u = flow[i, :, :, 0]\n",
    "        v = flow[i, :, :, 1]\n",
    "        for h in range(s[1]):\n",
    "            for w in range(s[1]):\n",
    "                ui = u[h,w]\n",
    "                vi = v[h,w]\n",
    "                img[ui, vi, :] = 255.\n",
    "        out.append(img)\n",
    "    return np.float32(np.uint8(out))\n",
    "\n",
    "\n",
    "def compute_color(u,v):\n",
    "    h, w = u.shape\n",
    "    img = np.zeros([h, w, 3])\n",
    "    nanIdx = np.isnan(u) | np.isnan(v)\n",
    "    u[nanIdx] = 0\n",
    "    v[nanIdx] = 0\n",
    "    # colorwheel = COLORWHEEL\n",
    "    colorwheel = make_color_wheel()\n",
    "    ncols = np.size(colorwheel, 0)\n",
    "    rad = np.sqrt(u**2+v**2)\n",
    "    a = np.arctan2(-v, -u) / np.pi\n",
    "    fk = (a+1) / 2 * (ncols - 1) + 1\n",
    "    k0 = np.floor(fk).astype(int)\n",
    "    k1 = k0 + 1\n",
    "    k1[k1 == ncols+1] = 1\n",
    "    f = fk - k0\n",
    "    for i in range(np.size(colorwheel,1)):\n",
    "        tmp = colorwheel[:, i]\n",
    "        col0 = tmp[k0-1] / 255\n",
    "        col1 = tmp[k1-1] / 255\n",
    "        col = (1-f) * col0 + f * col1\n",
    "        idx = rad <= 1\n",
    "        col[idx] = 1-rad[idx]*(1-col[idx])\n",
    "        notidx = np.logical_not(idx)\n",
    "        col[notidx] *= 0.75\n",
    "        img[:, :, i] = np.uint8(np.floor(255 * col*(1-nanIdx)))\n",
    "    return img\n",
    "\n",
    "\n",
    "def make_color_wheel():\n",
    "    RY, YG, GC, CB, BM, MR = (15, 6, 4, 11, 13, 6)\n",
    "    ncols = RY + YG + GC + CB + BM + MR\n",
    "    colorwheel = np.zeros([ncols, 3])\n",
    "    col = 0\n",
    "    # RY\n",
    "    colorwheel[0:RY, 0] = 255\n",
    "    colorwheel[0:RY, 1] = np.transpose(np.floor(255*np.arange(0, RY) / RY))\n",
    "    col += RY\n",
    "    # YG\n",
    "    colorwheel[col:col+YG, 0] = 255 - np.transpose(np.floor(255*np.arange(0, YG) / YG))\n",
    "    colorwheel[col:col+YG, 1] = 255\n",
    "    col += YG\n",
    "    # GC\n",
    "    colorwheel[col:col+GC, 1] = 255\n",
    "    colorwheel[col:col+GC, 2] = np.transpose(np.floor(255*np.arange(0, GC) / GC))\n",
    "    col += GC\n",
    "    # CB\n",
    "    colorwheel[col:col+CB, 1] = 255 - np.transpose(np.floor(255*np.arange(0, CB) / CB))\n",
    "    colorwheel[col:col+CB, 2] = 255\n",
    "    col += CB\n",
    "    # BM\n",
    "    colorwheel[col:col+BM, 2] = 255\n",
    "    colorwheel[col:col+BM, 0] = np.transpose(np.floor(255*np.arange(0, BM) / BM))\n",
    "    col += + BM\n",
    "    # MR\n",
    "    colorwheel[col:col+MR, 2] = 255 - np.transpose(np.floor(255 * np.arange(0, MR) / MR))\n",
    "    colorwheel[col:col+MR, 0] = 255\n",
    "    return colorwheel\n",
    "\n",
    "\n",
    "COLORWHEEL = make_color_wheel()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
